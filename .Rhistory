install.packages("tree")
library(tree)
library(ISLR)
attach(Carseats)
data(Carseats)
summary(Carseats)
library(tree)
library(ISLR)
data(Carseats)
summary(Carseats)
attach(Carseats)
High=ifelse(Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
tree.carseats=tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
library(tree)
library(ISLR)
attach(Carseats)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
load("C:/Snippet_GBM.RData")
load("C://ExampleModelData.RData")
load("C://ExampleModelData.RData")
workingData
WorkingData
DataExtractResults
DataExtractResults
DataExtractResults[1]
tbl<- DataExtractResults[1]
tbl<- data.frame(DataExtractResults[1])
View(tbl)
load(ExampleModelData.RData, verbose = FALSE)
setwd("C:/")
load(ExampleModelData.RData, verbose = FALSE)
setwd("C:/")
load(ExampleModelData.RData, verbose = FALSE)
load(ExampleModelData.RData)
load("ExampleModelData.RData")
tbl<- data.frame(DataExtractResults[1])
library(tree)
library(ISLR)
install.packages("ISLR")
data(Carseats)
summary(Carseats)
library(ISLR)
#####
# Begin Chapter 8.3 Lab: Decision Trees
#####
# Fitting Classification Trees
data(Carseats)
summary(Carseats)
# Sales is a continuous varibale, so recode as binary variable for classification
High=ifelse(Sales<=8,"No","Yes")
# use data.frame() function to merge High with rest of the data
Carseats=data.frame(Carseats,High)
# use tree() function to fit classification tree to predict High using all variables except Sales
tree.carseats=tree(High~.-Sales,Carseats)
# view training error rate
summary(tree.carseats)
library(tree)
library(ISLR)
#####
# Begin Chapter 8.3 Lab: Decision Trees
#####
# Fitting Classification Trees
data(Carseats)
summary(Carseats)
# Sales is a continuous varibale, so recode as binary variable for classification
High=ifelse(Sales<=8,"No","Yes")
# use data.frame() function to merge High with rest of the data
Carseats=data.frame(Carseats,High)
# use tree() function to fit classification tree to predict High using all variables except Sales
tree.carseats=tree(High~.-Sales,Carseats)
library(tree)
library(ISLR)
#####
# Begin Chapter 8.3 Lab: Decision Trees
#####
# Fitting Classification Trees
data(Carseats)
summary(Carseats)
# Sales is a continuous varibale, so recode as binary variable for classification
High=ifelse(Sales<=8,"No","Yes")
High=ifelse(Carseats$Sales<=8,"No","Yes")
Carseats=data.frame(Carseats,High)
# use tree() function to fit classification tree to predict High using all variables except Sales
tree.carseats=tree(High~.-Sales,Carseats)
# view training error rate
summary(tree.carseats)
plot(tree.carseats)
# The argument pretty = 0 instructs R to include the category names for any qualitative predictors,
# rather than simply displaying a letter for each category
text(tree.carseats,pretty=0, all=TRUE, splits=TRUE)
# get branch info
tree.carseats
# create test set
set.seed(2)
train=sample(1:nrow(Carseats), 200)
Carseats.test=Carseats[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
# correct predictions 71.5%, 57 incorrect predictions (nominal error rate)
table(tree.pred,High.test)
(86+57)/200
# try pruning (cross-validation)
set.seed(3)
# We use the argument FUN = prune.misclass in order to indicate that we want the
# classification error rate to guide the cross-validation and pruning process,
# rather than the default for the cv.tree() function, which is deviance.
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
# k parameter retuned in cost-complexity, dev is nominal error rate (# mis-classified)
# tree with lowest error rate has 9 nodes
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# tree with lowest error rate from cross-validation had 9 nodes
# so we will use this for pruning
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(94+60)/200 # now 77% correctly classified (46 incorrectly classified)
# test larger tree -> lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(86+62)/200
# (a) Split the data set into a training set and a test set.
set.seed(93)
train = sample(1:nrow(Carseats), nrow(Carseats)/2)
####
# (b) Fit a regression tree to the training set. Plot the tree, and interpret the results.
# What test MSE do you obtain?
library(MASS)
set.seed(1)
carRegrTree =tree(Sales~. -High, Carseats, subset=train)
# Note: MSE = Residual Mean Deviance found in summary output
# deviance = sum of squared errors
summary(carRegrTree)
par(mfrow=c(1,1))
plot(carRegrTree)
text(carRegrTree,pretty=0, all=T, splits=T)
####
# (c) cross-validation
cvRegrTree=cv.tree(carRegrTree)
plot(cvRegrTree$size,cvRegrTree$dev,type='b')
# check exact values
cvRegrTree$size
cvRegrTree$dev
# check out pruned tree with 8 terminal nodes
pruneRegr=prune.tree(carRegrTree,best=8)
plot(pruneRegr)
text(pruneRegr,pretty=0, all=T)
summary(pruneRegr)
library(randomForest)
set.seed(1)
####
# bagging model
# The argument mtry = 10 indicates that all 10 predictors should be considered
# for each split of the tree â€” in other words, that bagging should be done.
carBag=randomForest(Sales~. -High, data=Carseats, subset=train, mtry=10, importance=TRUE)
# show model results
carBag
# calculate predictions on test set
yhat.bag = predict(carBag, newdata=Carseats[-train,])
# create vector of response (Sales) for test set for plotting
car.test <- Carseats[-train,"Sales"]
plot(yhat.bag, car.test)
abline(0,1)
# calc MSE
mean((yhat.bag-car.test)^2)
# calc importance
importance(carBag)
varImpPlot(carBag)
# this code not used (from textbook example) - reduces number of trees
# bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,ntree=25)
# yhat.bag = predict(bag.boston,newdata=Boston[-train,])
# mean((yhat.bag-boston.test)^2)
####
# (e) Use random forests to analyze this data. What test MSE do you obtain?
# Use the importance() function to determine which variables are most important.
# Describe the effect of m, the number of variables considered at each split,
# on the error rate obtained.
####
# random forest model
Carseats[,-which(names(Carseats) %in% c("Sales","High"))]
set.seed(1)
# use default setting of 'mtry' parameter for random forest, p/3
rfCar.default=randomForest(Sales~. -High ,data=Carseats, subset=train,importance=TRUE)
rfCar.default
set.seed(1)
# set mtry = 4 to see what happens
rfCar=randomForest(Sales~. -High ,data=Carseats, subset=train, mtry=4,importance=TRUE)
rfCar
set.seed(1)
# use the tuneRF function to search and plot different values of mtry
tuneRF(Carseats[,-which(names(Carseats) %in% c("Sales","High"))], Carseats[,"Sales"],
stepFactor=2, mtryStart=2, doBest=T, plot=T)
yhat.rf = predict(rfCar.default,newdata=Carseats[-train,])
# create vector of response (Sales) for test set for plotting
car.test <- Carseats[-train,"Sales"]
# calc test MSE
mean((yhat.rf-car.test)^2)
importance(rfCar.default)
varImpPlot(rfCar.default)
#####
# End Exercise 8 of Section 8.4
# Begin Exercise 9 of Section 8.4
#####
data(OJ)
summary(OJ)
plot(OJ$Purchase,OJ$PriceCH)
set.seed(1013)
# part (a)
# create training and test sets
train = sample(dim(OJ)[1], 800)
trainOJ = OJ[train, ]
testOJ = OJ[-train, ]
# part (b)
# fit tree
library(tree)
oj.tree = tree(Purchase ~ ., data = trainOJ)
summary(oj.tree)
# part c
# get detailed info
oj.tree
# part d
# plot
par(mfrow=c(1,1))
plot(oj.tree)
text(oj.tree, pretty = 0, all=T, splits=T)
# part e
# predict response on test data, produce confusion matrix
oj.pred = predict(oj.tree, testOJ, type = "class")
table(testOJ$Purchase, oj.pred)
# part f
# cross validation
cv.oj = cv.tree(oj.tree, FUN=prune.misclass)
cv.oj
par(mfrow=c(1,2))
plot(cv.oj$size,cv.oj$dev,type="b")
